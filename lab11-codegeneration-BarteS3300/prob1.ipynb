{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\barte\\AppData\\Roaming\\Python\\Python310\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function Name: sort_array -> Category: sorting\n",
      "Function Name: binary_search -> Category: searching\n",
      "Function Name: calculate_sum -> Category: mathematical operations\n",
      "Function Name: linked_list_operations -> Category: data structures\n",
      "Function Name: show_prediction_labels_on_image -> Category: machine learning\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "# Initialize a pre-trained model\n",
    "model = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n",
    "\n",
    "# List of category descriptions\n",
    "category_descriptions = {\n",
    "    \"sorting\": \"sorting elements, bubble sort, quick sort, merge sort, heap sort\",\n",
    "    \"searching\": \"searching elements, binary search, linear search, search algorithms\",\n",
    "    \"mathematical operations\": \"mathematical operations, addition, subtraction, trigonometry, calculus\",\n",
    "    \"data structures\": \"data structures, linked list, stack, queue, tree, graph\",\n",
    "    \"string manipulation\": \"manipulating strings, string operations, string matching, regular expressions\",\n",
    "    \"file handling\": \"file handling, reading files, writing files, file operations\",\n",
    "    \"networking\": \"networking, socket programming, network protocols, client-server communication\",\n",
    "    \"web scraping\": \"web scraping, parsing HTML, extracting data from websites, web crawling\",\n",
    "    \"machine learning\": \"machine learning, deep learning, neural networks, training models, prediction\",\n",
    "    \"other\": \"miscellaneous functions\"\n",
    "}\n",
    "\n",
    "# Encode category descriptions\n",
    "category_embeddings = {category: model.encode(description, convert_to_tensor=True) for category, description in category_descriptions.items()}\n",
    "\n",
    "cache_file = 'func_name_to_category_cache.json'\n",
    "\n",
    "# Load cached mappings if available\n",
    "if os.path.exists(cache_file):\n",
    "    with open(cache_file, 'r') as f:\n",
    "        func_name_to_category = json.load(f)\n",
    "else:\n",
    "    func_name_to_category = {}\n",
    "\n",
    "def map_func_name_to_category(func_name):\n",
    "    if func_name in func_name_to_category:\n",
    "        return func_name_to_category[func_name]\n",
    "    \n",
    "    func_name_embedding = model.encode(func_name, convert_to_tensor=True)\n",
    "    similarities = {category: util.pytorch_cos_sim(func_name_embedding, embedding).item() for category, embedding in category_embeddings.items()}\n",
    "    category = max(similarities, key=similarities.get)\n",
    "    func_name_to_category[func_name] = category\n",
    "\n",
    "    # Save the updated mappings to the cache file\n",
    "    with open(cache_file, 'w') as f:\n",
    "        json.dump(func_name_to_category, f)\n",
    "    \n",
    "    return category\n",
    "\n",
    "# Test the mapping function with some example function names\n",
    "func_names = [\"sort_array\", \"binary_search\", \"calculate_sum\", \"linked_list_operations\", \"show_prediction_labels_on_image\"]\n",
    "\n",
    "for func_name in func_names:\n",
    "    category = map_func_name_to_category(func_name)\n",
    "    print(f\"Function Name: {func_name} -> Category: {category}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\barte\\AppData\\Roaming\\Python\\Python310\\site-packages\\datasets\\load.py:1491: FutureWarning: The repository for code_search_net contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/code_search_net\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16000/16000 [00:04<00:00, 3930.86 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 861/861 [00:00<00:00, 5280.92 examples/s]\n",
      "c:\\Users\\barte\\miniconda3\\envs\\py310\\lib\\site-packages\\torch\\_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at microsoft/codebert-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\barte\\AppData\\Roaming\\Python\\Python310\\site-packages\\transformers\\training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "  5%|â–Œ         | 100/2000 [00:25<08:01,  3.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.0916, 'grad_norm': 5.750487327575684, 'learning_rate': 1.9e-05, 'epoch': 0.1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|â–ˆ         | 200/2000 [00:53<08:27,  3.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.0054, 'grad_norm': 5.591728210449219, 'learning_rate': 1.8e-05, 'epoch': 0.2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|â–ˆâ–Œ        | 300/2000 [01:23<07:44,  3.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.8736, 'grad_norm': 8.125833511352539, 'learning_rate': 1.7e-05, 'epoch': 0.3}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|â–ˆâ–ˆ        | 400/2000 [02:08<12:11,  2.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.7739, 'grad_norm': 8.508023262023926, 'learning_rate': 1.6000000000000003e-05, 'epoch': 0.4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|â–ˆâ–ˆâ–Œ       | 500/2000 [02:54<11:34,  2.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.674, 'grad_norm': 9.839591979980469, 'learning_rate': 1.5000000000000002e-05, 'epoch': 0.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|â–ˆâ–ˆâ–ˆ       | 600/2000 [03:47<10:40,  2.19it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.664, 'grad_norm': 11.845390319824219, 'learning_rate': 1.4e-05, 'epoch': 0.6}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 700/2000 [04:32<08:58,  2.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.5995, 'grad_norm': 10.147231101989746, 'learning_rate': 1.3000000000000001e-05, 'epoch': 0.7}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 800/2000 [07:14<09:01,  2.22it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.6077, 'grad_norm': 9.830946922302246, 'learning_rate': 1.2e-05, 'epoch': 0.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 900/2000 [07:59<08:17,  2.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.6122, 'grad_norm': 10.988487243652344, 'learning_rate': 1.1000000000000001e-05, 'epoch': 0.9}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1000/2000 [08:45<07:38,  2.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.5474, 'grad_norm': 10.532018661499023, 'learning_rate': 1e-05, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1000/2000 [09:00<07:38,  2.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.6047215461730957, 'eval_runtime': 8.5249, 'eval_samples_per_second': 100.998, 'eval_steps_per_second': 6.334, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1100/2000 [09:46<06:53,  2.17it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4512, 'grad_norm': 12.35280704498291, 'learning_rate': 9e-06, 'epoch': 1.1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1200/2000 [10:32<06:10,  2.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4314, 'grad_norm': 16.215505599975586, 'learning_rate': 8.000000000000001e-06, 'epoch': 1.2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1300/2000 [11:19<05:26,  2.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4322, 'grad_norm': 10.061102867126465, 'learning_rate': 7e-06, 'epoch': 1.3}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 1400/2000 [12:05<04:38,  2.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3665, 'grad_norm': 14.795998573303223, 'learning_rate': 6e-06, 'epoch': 1.4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1500/2000 [12:50<03:22,  2.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3061, 'grad_norm': 11.171121597290039, 'learning_rate': 5e-06, 'epoch': 1.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1600/2000 [21:15<02:57,  2.26it/s]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3835, 'grad_norm': 14.642250061035156, 'learning_rate': 4.000000000000001e-06, 'epoch': 1.6}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1700/2000 [22:00<02:17,  2.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.368, 'grad_norm': 15.009904861450195, 'learning_rate': 3e-06, 'epoch': 1.7}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 1800/2000 [22:46<01:32,  2.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3711, 'grad_norm': 14.178555488586426, 'learning_rate': 2.0000000000000003e-06, 'epoch': 1.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 1900/2000 [23:33<00:45,  2.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3772, 'grad_norm': 15.135271072387695, 'learning_rate': 1.0000000000000002e-06, 'epoch': 1.9}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2000/2000 [24:20<00:00,  2.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.403, 'grad_norm': 14.262889862060547, 'learning_rate': 0.0, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2000/2000 [24:35<00:00,  1.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.555184006690979, 'eval_runtime': 8.3436, 'eval_samples_per_second': 103.193, 'eval_steps_per_second': 6.472, 'epoch': 2.0}\n",
      "{'train_runtime': 1475.6221, 'train_samples_per_second': 21.686, 'train_steps_per_second': 1.355, 'train_loss': 1.5669732208251954, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 54/54 [00:08<00:00,  6.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation results: {'eval_loss': 1.555184006690979, 'eval_runtime': 8.0904, 'eval_samples_per_second': 106.423, 'eval_steps_per_second': 6.675, 'epoch': 2.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('p1_saved_tokenizer\\\\tokenizer_config.json',\n",
       " 'p1_saved_tokenizer\\\\special_tokens_map.json',\n",
       " 'p1_saved_tokenizer\\\\vocab.json',\n",
       " 'p1_saved_tokenizer\\\\merges.txt',\n",
       " 'p1_saved_tokenizer\\\\added_tokens.json',\n",
       " 'p1_saved_tokenizer\\\\tokenizer.json')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Define the mapping between function names and categories\n",
    "label_mapping = {\n",
    "    \"sorting\": 0,\n",
    "    \"searching\": 1,\n",
    "    \"mathematical operations\": 2,\n",
    "    \"data structures\": 3,\n",
    "    \"string manipulation\": 4,\n",
    "    \"file handling\": 5,\n",
    "    \"networking\": 6,\n",
    "    \"web scraping\": 7,\n",
    "    \"machine learning\": 8,\n",
    "    \"other\": 9\n",
    "}\n",
    "\n",
    "# Load the CodeSearchNet dataset\n",
    "dataset = load_dataset(\"code_search_net\", \"python\")\n",
    "\n",
    "# Pre-process the dataset\n",
    "def preprocess_function(examples):\n",
    "    examples[\"label\"] = [label_mapping.get(map_func_name_to_category(func), label_mapping[\"other\"]) for func in examples[\"func_name\"]]\n",
    "    return tokenizer(examples[\"whole_func_string\"], truncation=True, padding=\"max_length\", max_length=128)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/codebert-base\")\n",
    "\n",
    "# Select a shard of the dataset for quicker processing\n",
    "shard_size = 16000  # Define the shard size\n",
    "train_dataset = dataset['train'].shard(index=0, num_shards=len(dataset[\"train\"]) / shard_size)\n",
    "validation_dataset = dataset['test'].shard(index=0, num_shards=len(dataset[\"train\"]) / shard_size)\n",
    "\n",
    "# Tokenize the datasets\n",
    "tokenized_train_datasets = train_dataset.map(preprocess_function, batched=True, batch_size=1000)\n",
    "tokenized_validation_datasets = validation_dataset.map(preprocess_function, batched=True, batch_size=1000)\n",
    "\n",
    "# Load pre-trained model\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"microsoft/codebert-base\", num_labels=len(label_mapping))\n",
    "\n",
    "# Define the training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=2,\n",
    "    logging_steps=100,\n",
    "    weight_decay=0.01,\n",
    ")\n",
    "\n",
    "# Define the trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train_datasets,\n",
    "    eval_dataset=tokenized_validation_datasets,\n",
    ")\n",
    "\n",
    "# Fine-tune the model\n",
    "trainer.train()\n",
    "\n",
    "# Evaluate the model\n",
    "stats = trainer.evaluate()\n",
    "print(f\"Stats of the trained model: {stats}\")\n",
    "\n",
    "# Save the model\n",
    "model.save_pretrained(\"p1Model\")\n",
    "tokenizer.save_pretrained(\"p1Tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Code Snippet: sorted(l, key=lambda x: (-int(x[1]), x[0]))\n",
      "Classification: sorting, Score: 0.7280794382095337\n",
      "\n",
      "Code Snippet: def multiply(a, b): return a * b\n",
      "Classification: mathematical operations, Score: 0.3808503746986389\n",
      "\n",
      "Code Snippet: def divide(a, b): return a / b\n",
      "Classification: mathematical operations, Score: 0.3947147727012634\n",
      "\n",
      "Code Snippet: words = text.split(' ') \\ last = words[0] \\ for word in words: \\ if word > last: \\ last = word \\ return last\n",
      "Classification: string manipulation, Score: 0.5963256359100342\n",
      "\n",
      "Code Snippet: def loss_fn(y_true, y_pred): return tf.reduce_mean(tf.square(y_true - y_pred))\n",
      "Classification: other, Score: 0.2912917733192444\n",
      "\n",
      "Code Snippet: def func(x, y): return x * y + x\n",
      "Classification: other, Score: 0.5813041925430298\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline\n",
    "\n",
    "# Define the label mapping\n",
    "labels = {\n",
    "    0: \"sorting\",\n",
    "    1: \"searching\",\n",
    "    2: \"mathematical operations\",\n",
    "    3: \"data structures\",\n",
    "    4: \"string manipulation\",\n",
    "    5: \"file handling\",\n",
    "    6: \"networking\",\n",
    "    7: \"web scraping\",\n",
    "    8: \"machine learning\",\n",
    "    9: \"other\"\n",
    "}\n",
    "\n",
    "# Load the fine-tuned model and tokenizer\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"p1Model\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"p1Tokenizer\")\n",
    "\n",
    "# Create a pipeline for text classification\n",
    "classifier = pipeline(\"text-classification\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "# Define the code snippets\n",
    "code_snippets = [\n",
    "    \"sorted(l, key=lambda x: (-int(x[1]), x[0]))\",\n",
    "    \"def multiply(a, b): return a * b\",\n",
    "    \"def divide(a, b): return a / b\",\n",
    "    \"words = text.split(' ') \\\\ last = words[0] \\\\ for word in words: \\\\ if word > last: \\\\ last = word \\\\ return last\",\n",
    "    \"def loss_fn(y_true, y_pred): return tf.reduce_mean(tf.square(y_true - y_pred))\",\n",
    "    \"def func(x, y): return x * y + x\"\n",
    "]\n",
    "\n",
    "# Classify the code snippets\n",
    "for code_snippet in code_snippets:\n",
    "    # Classify the code snippet\n",
    "    classification_result = classifier(code_snippet)\n",
    "\n",
    "    # Extract the label and map it to the class name\n",
    "    label = int(classification_result[0]['label'].split('_')[-1])  # Extract numerical label from 'LABEL_0', 'LABEL_1', etc.\n",
    "    class_name = labels.get(label, \"Unknown\")\n",
    "\n",
    "    print(f\"Code Snippet: {code_snippet}\")\n",
    "    print(f\"Classification: {class_name}, Score: {classification_result[0]['score']}\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
