{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\barte\\AppData\\Roaming\\Python\\Python310\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': ['question_id', 'parent_answer_post_id', 'prob', 'snippet', 'intent', 'rewritten_intent', 'id']}\n",
      "{'question_id': 34705205, 'parent_answer_post_id': 34705233, 'prob': 0.8690001442846342, 'snippet': 'sorted(l, key=lambda x: (-int(x[1]), x[0]))', 'intent': 'Sort a nested list by two elements', 'rewritten_intent': \"sort a nested list l by two elements '1' and '0'\", 'id': '34705205_34705233_0'}\n",
      "{'question_id': 13905936, 'parent_answer_post_id': 13905946, 'prob': 0.8526701436370034, 'snippet': '[int(x) for x in str(num)]', 'intent': 'converting integer to list in python', 'rewritten_intent': 'convert integer num to list', 'id': '13905936_13905946_0'}\n",
      "{'question_id': 13837848, 'parent_answer_post_id': 13838041, 'prob': 0.8521431843789492, 'snippet': \"c.decode('unicode_escape')\", 'intent': 'Converting byte string in unicode string', 'rewritten_intent': 'convert byte string c to unicode string', 'id': '13837848_13838041_0'}\n",
      "{'question_id': 23490152, 'parent_answer_post_id': 23490179, 'prob': 0.850829261966626, 'snippet': \"parser.add_argument('-t', dest='table', help='', nargs='+')\", 'intent': 'List of arguments with argparse', 'rewritten_intent': \"add argument '-t' to argparse parser parser with dest 'table', help '' and nargs '+'\", 'id': '23490152_23490179_0'}\n",
      "{'question_id': 2721782, 'parent_answer_post_id': 2721807, 'prob': 0.8403723482242028, 'snippet': \"datetime.datetime.strptime(s, '%Y-%m-%dT%H:%M:%SZ')\", 'intent': 'How to convert a Date string to a DateTime object?', 'rewritten_intent': 'convert a date string s to a datetime object', 'id': '2721782_2721807_0'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1782/1782 [00:00<00:00, 3261.05 examples/s]\n",
      "Map: 100%|██████████| 198/198 [00:00<00:00, 3089.97 examples/s]\n",
      "C:\\Users\\barte\\AppData\\Roaming\\Python\\Python310\\site-packages\\transformers\\training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 50/560 [00:35<04:25,  1.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4158, 'grad_norm': 0.3081796169281006, 'learning_rate': 0.0018285714285714285, 'epoch': 0.45}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 100/560 [01:02<04:00,  1.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4053, 'grad_norm': 0.3990699350833893, 'learning_rate': 0.00165, 'epoch': 0.89}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 150/560 [01:37<05:30,  1.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3552, 'grad_norm': 0.23366963863372803, 'learning_rate': 0.0014714285714285717, 'epoch': 1.34}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▌      | 200/560 [02:16<05:05,  1.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3428, 'grad_norm': 0.2951486110687256, 'learning_rate': 0.001292857142857143, 'epoch': 1.79}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▍     | 250/560 [03:14<07:25,  1.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3091, 'grad_norm': 0.1736857146024704, 'learning_rate': 0.0011142857142857144, 'epoch': 2.23}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      " 45%|████▍     | 250/560 [03:18<07:25,  1.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.33878087997436523, 'eval_runtime': 4.4906, 'eval_samples_per_second': 44.092, 'eval_steps_per_second': 2.895, 'epoch': 2.23}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|█████▎    | 300/560 [04:37<06:19,  1.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2938, 'grad_norm': 0.21036379039287567, 'learning_rate': 0.0009357142857142857, 'epoch': 2.68}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▎   | 350/560 [05:47<03:19,  1.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2716, 'grad_norm': 0.28615134954452515, 'learning_rate': 0.0007571428571428572, 'epoch': 3.12}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|███████▏  | 400/560 [06:34<02:28,  1.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2489, 'grad_norm': 0.21100544929504395, 'learning_rate': 0.0005785714285714286, 'epoch': 3.57}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 450/560 [07:19<01:23,  1.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2538, 'grad_norm': 0.1764555275440216, 'learning_rate': 0.0004, 'epoch': 4.02}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|████████▉ | 500/560 [08:10<01:01,  1.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2191, 'grad_norm': 0.20747998356819153, 'learning_rate': 0.00022142857142857142, 'epoch': 4.46}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      " 89%|████████▉ | 500/560 [08:13<01:01,  1.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.3472926616668701, 'eval_runtime': 3.2426, 'eval_samples_per_second': 61.062, 'eval_steps_per_second': 4.009, 'epoch': 4.46}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|█████████▊| 550/560 [09:13<00:13,  1.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.229, 'grad_norm': 0.2352881133556366, 'learning_rate': 4.2857142857142856e-05, 'epoch': 4.91}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 560/560 [09:25<00:00,  1.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 565.8606, 'train_samples_per_second': 15.746, 'train_steps_per_second': 0.99, 'train_loss': 0.3919362540755953, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13/13 [00:04<00:00,  3.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stats of the trained model: {'eval_loss': 0.34741973876953125, 'eval_runtime': 4.1381, 'eval_samples_per_second': 47.848, 'eval_steps_per_second': 3.142, 'epoch': 5.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('./p2Tokenizer\\\\tokenizer_config.json',\n",
       " './p2Tokenizer\\\\special_tokens_map.json',\n",
       " './p2Tokenizer\\\\spiece.model',\n",
       " './p2Tokenizer\\\\added_tokens.json')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration, Trainer, TrainingArguments\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Load pre-trained T5 model and tokenizer\n",
    "model_name = \"t5-small\"\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "# Load the CoNaLa dataset for code summarization\n",
    "dataset = load_dataset(\"codeparrot/conala-mined-curated\")\n",
    "\n",
    "print(dataset.column_names)\n",
    "# Print the first 5 instances from the dataset\n",
    "for i in range(5):\n",
    "    print(dataset['train'][i])\n",
    "\n",
    "# Preprocess the dataset\n",
    "def tokenize_function(examples):\n",
    "    # Use the 'snippet' field for input and 'intent' field for output\n",
    "    inputs = tokenizer([\"summarize: \" + code for code in examples[\"snippet\"]], truncation=True, padding=\"max_length\", max_length=512)\n",
    "    labels = tokenizer(examples[\"rewritten_intent\"], truncation=True, padding=\"max_length\", max_length=128)\n",
    "    inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return inputs\n",
    "\n",
    "# Split the training data into training and validation sets\n",
    "dataset = dataset['train'].train_test_split(test_size=0.1)\n",
    "\n",
    "train_dataset = dataset['train'].shard(index=0, num_shards=300)\n",
    "validation_dataset = dataset['test'].shard(index=0, num_shards=300)\n",
    "\n",
    "# Tokenize the datasets\n",
    "tokenized_train_datasets = train_dataset.map(tokenize_function, batched=True, batch_size=1000)\n",
    "tokenized_validation_datasets = validation_dataset.map(tokenize_function, batched=True, batch_size=1000)\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=250,  # Evaluate every 250 steps\n",
    "    logging_steps=50,\n",
    "    learning_rate=2e-3,\n",
    "    per_device_train_batch_size=16,  # Reduced batch size\n",
    "    per_device_eval_batch_size=16,  # Reduced batch size\n",
    "    num_train_epochs=5,  # Increased number of epochs\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    save_total_limit=4,\n",
    "    save_steps=500,\n",
    "    fp16=True,  # Enable mixed precision training\n",
    ")\n",
    "\n",
    "# Move model to GPU if available\n",
    "if torch.cuda.is_available():\n",
    "    model.cuda()\n",
    "    print(\"Using GPU\")\n",
    "\n",
    "# Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train_datasets,\n",
    "    eval_dataset=tokenized_validation_datasets,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "# Fine-tune the model\n",
    "trainer.train()\n",
    "\n",
    "# Evaluate the model\n",
    "stats = trainer.evaluate()\n",
    "\n",
    "print(f\"Stats of the trained model: {stats}\")\n",
    "\n",
    "# Save the model and tokenizer\n",
    "model.save_pretrained(\"./p2Model\")\n",
    "tokenizer.save_pretrained(\"./p2Tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Code snippet 1 summary: sort list l by key 1\n",
      "Code snippet 2 summary: multiply a list a by two elements\n",
      "Code snippet 3 summary: divide a list a by b\n",
      "Code snippet 4 summary: split string words into two parts\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
    "\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"./p2Model\")\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"./p2Tokenizer\")\n",
    "\n",
    "def generate_summary(model, tokenizer, code_snippet):\n",
    "    # Preprocess the code snippet\n",
    "    inputs = tokenizer.encode(\"summarize: \" + code_snippet, return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "    # Move inputs and model to GPU if available\n",
    "    if torch.cuda.is_available():\n",
    "        inputs = inputs.cuda()\n",
    "        model = model.cuda()\n",
    "    # Generate summary\n",
    "    summary_ids = model.generate(inputs, max_length=128, num_beams=4, early_stopping=True)\n",
    "    # Decode the summary\n",
    "    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "    return summary\n",
    "\n",
    "# Example code snippet\n",
    "code_snippets = [\"sorted(l, key=lambda x: (-int(x[1]), x[0]))\",\n",
    "                 \"def multiply(a, b): return a * b\",\n",
    "                 \"def divide(a, b): return a / b\",\n",
    "                 \"words = text.split(' ') \\ last = words[0] \\ for word in words: \\ if word > last: \\ last = word \\ return last\"]\n",
    "for i, code_snippet in enumerate(code_snippets):\n",
    "    summary = generate_summary(model, tokenizer, code_snippet)\n",
    "    print(f\"Code snippet {i+1} summary: {summary}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
